{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import logging\n",
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "logging.basicConfig()\n",
    "\n",
    "# Set paths\n",
    "folder = \"/Documents/CS838/Stage3/datasets\"\n",
    "table_a_file = \"songs.csv\"\n",
    "table_b_file = \"tracks.csv\"\n",
    "\n",
    "# Set directory path\n",
    "datasets_dir = expanduser(\"~\") + folder\n",
    "\n",
    "# Set path for tables\n",
    "path_A = datasets_dir + os.sep + table_a_file\n",
    "path_B = datasets_dir + os.sep + table_b_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "# Read csv files\n",
    "A = em.read_csv_metadata(path_A, key='id')\n",
    "B = em.read_csv_metadata(path_B, key='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tuples in A: 961593\n",
      "Number of tuples in B: 734485\n",
      "Number of tuples in A X B (i.e the cartesian product): 706275634605\n"
     ]
    }
   ],
   "source": [
    "print('Number of tuples in A: ' + str(len(A)))\n",
    "print('Number of tuples in B: ' + str(len(B)))\n",
    "print('Number of tuples in A X B (i.e the cartesian product): ' + str(len(A)*len(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:04:32 | ETA: 00:04:42 | ETA: 00:04:58 | ETA: 00:04:45 | ETA: 00:04:35 | ETA: 00:04:20 | ETA: 00:04:11 | ETA: 00:04:00 | ETA: 00:03:47 | ETA: 00:03:33 | ETA: 00:03:26 | ETA: 00:03:16 | ETA: 00:03:06 | ETA: 00:02:54 | ETA: 00:02:41 | ETA: 00:02:28 | ETA: 00:02:15 | ETA: 00:02:05 | ETA: 00:01:55 | ETA: 00:01:45 | ETA: 00:01:34 | ETA: 00:01:23 | ETA: 00:01:13 | ETA: 00:01:01 | ETA: 00:00:51 | ETA: 00:00:41 | ETA: 00:00:30 | ETA: 00:00:20 | ETA: 00:00:10 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:05:06\n"
     ]
    }
   ],
   "source": [
    "# Downsample tables\n",
    "sample_A, sample_B = em.down_sample(A, B, 10000, 1.5, show_progress=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save downsampled tables to csv\n",
    "sample_A.to_csv(datasets_dir + os.sep + \"sampleA.csv\")\n",
    "sample_B.to_csv(datasets_dir + os.sep + \"sampleB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>episode</th>\n",
       "      <th>song</th>\n",
       "      <th>artists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>505168</th>\n",
       "      <td>276654</td>\n",
       "      <td>The Tonight Show Starring Johnny Carson</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>(1989-11-14)</td>\n",
       "      <td>Johnnys Theme</td>\n",
       "      <td>paul anka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226722</th>\n",
       "      <td>437417</td>\n",
       "      <td>Father of the Bird</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Flying Trapeze</td>\n",
       "      <td>gaston lyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641892</th>\n",
       "      <td>608033</td>\n",
       "      <td>Secretary</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Its So Strange (The Way Love Works)</td>\n",
       "      <td>the honeydogs+jeff barry+ellie greenwich+neil diamond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406210</th>\n",
       "      <td>170096</td>\n",
       "      <td>OchÌ©ntame... otra vez</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Vidas de copla (#2.10)</td>\n",
       "      <td>Torbellino de colores</td>\n",
       "      <td>lola flores+rafael de leÌ_n+juan solano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429181</th>\n",
       "      <td>595711</td>\n",
       "      <td>River of No Return</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Down in the Meadow</td>\n",
       "      <td>marilyn monroe+lionel newman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                    title    year  \\\n",
       "505168  276654  The Tonight Show Starring Johnny Carson  1962.0   \n",
       "226722  437417                       Father of the Bird  1997.0   \n",
       "641892  608033                                Secretary  2002.0   \n",
       "406210  170096                   OchÌ©ntame... otra vez  2014.0   \n",
       "429181  595711                       River of No Return  1954.0   \n",
       "\n",
       "                       episode                                 song  \\\n",
       "505168            (1989-11-14)                        Johnnys Theme   \n",
       "226722                     NaN                   The Flying Trapeze   \n",
       "641892                     NaN  Its So Strange (The Way Love Works)   \n",
       "406210  Vidas de copla (#2.10)                Torbellino de colores   \n",
       "429181                     NaN                   Down in the Meadow   \n",
       "\n",
       "                                                      artists  \n",
       "505168                                              paul anka  \n",
       "226722                                            gaston lyle  \n",
       "641892  the honeydogs+jeff barry+ellie greenwich+neil diamond  \n",
       "406210                lola flores+rafael de leÌ_n+juan solano  \n",
       "429181                           marilyn monroe+lionel newman  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:01 | ETA: 00:00:06 | ETA: 00:00:04 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "# Perform 1-word overlap blocking on artist name\n",
    "ob = em.OverlapBlocker()\n",
    "C1 = ob.block_tables(sample_A, sample_B, 'artist_name', 'artists', \n",
    "                     rem_stop_words=True, word_level=True, overlap_size=1, \n",
    "                     l_output_attrs=['id', 'title', 'artist_name', 'year'], \n",
    "                     r_output_attrs=['id', 'title', 'year', 'episode', 'song', 'artists'],\n",
    "                     show_progress=True)\n",
    "C1.to_csv(datasets_dir + os.sep + \"C1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:09 | ETA: 00:00:11 | ETA: 00:00:11 | ETA: 00:00:11 | ETA: 00:00:11 | ETA: 00:00:11 | ETA: 00:00:10 | ETA: 00:00:10 | ETA: 00:00:09 | ETA: 00:00:08 | ETA: 00:00:08 | ETA: 00:00:07 | ETA: 00:00:07 | ETA: 00:00:07 | ETA: 00:00:06 | ETA: 00:00:05 | ETA: 00:00:05 | ETA: 00:00:05 | ETA: 00:00:04 | ETA: 00:00:04 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:12\n"
     ]
    }
   ],
   "source": [
    "# Perform 1-word overlap blocking on title\n",
    "C2 = ob.block_tables(sample_A, sample_B, 'title', 'song', \n",
    "                     rem_stop_words=True, word_level=True, overlap_size=1, \n",
    "                     l_output_attrs=['id', 'title', 'artist_name', 'year'], \n",
    "                     r_output_attrs=['id', 'title', 'year', 'episode', 'song', 'artists'], \n",
    "                     show_progress=True)\n",
    "C2.to_csv(datasets_dir + os.sep + \"C2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine blocker outputs for 1 work overlap on artist and title\n",
    "C = em.combine_blocker_outputs_via_union([C1, C2])\n",
    "C.to_csv(datasets_dir + os.sep + \"C.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "# Read in files as a hack to fix KeyError (explained in writeup)\n",
    "path_sA = datasets_dir + os.sep + \"sampleA.csv\"\n",
    "path_sB = datasets_dir + os.sep + \"sampleB.csv\"\n",
    "path_C = datasets_dir + os.sep + \"C.csv\"\n",
    "\n",
    "# Read and set metadata for downsampled B\n",
    "sA = em.read_csv_metadata(path_sA, key='id')\n",
    "\n",
    "# Read and set metadata for downsampled B\n",
    "sB = em.read_csv_metadata(path_sB, key='id')\n",
    "\n",
    "# Read and set metadata for blocked C1\n",
    "C = em.read_csv_metadata(path_C, key='_id', ltable=sA, rtable=sB, fk_ltable='ltable_id', fk_rtable='rtable_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Debug overlap blocker\n",
    "corres = [('title','song'), ('artist_name','artists'), ('year', 'year')]\n",
    "dbg = em.debug_blocker(C, sA, sB, output_size=200, attr_corres=corres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbg.to_csv(datasets_dir + os.sep + 'dbg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample candidate set\n",
    "S = em.sample_table(C, 450)\n",
    "S.to_csv(datasets_dir + os.sep + 'S.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'id'), ('title', 'title'), ('year', 'year')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Too many non-matches so try rule based blocking blocking\n",
    "block_t = em.get_tokenizers_for_blocking()\n",
    "block_s = em.get_sim_funs_for_blocking()\n",
    "block_c = em.get_attr_corres(sample_A, sample_B)\n",
    "block_c['corres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('title', 'song'), ('artist_name', 'artists'), ('year', 'year')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_c['corres'] = corres\n",
    "block_c['corres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>left_attribute</th>\n",
       "      <th>right_attribute</th>\n",
       "      <th>left_attr_tokenizer</th>\n",
       "      <th>right_attr_tokenizer</th>\n",
       "      <th>simfunction</th>\n",
       "      <th>function</th>\n",
       "      <th>function_source</th>\n",
       "      <th>is_auto_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title_song_jac_qgm_3_qgm_3</td>\n",
       "      <td>title</td>\n",
       "      <td>song</td>\n",
       "      <td>qgm_3</td>\n",
       "      <td>qgm_3</td>\n",
       "      <td>jaccard</td>\n",
       "      <td>&lt;function title_song_jac_qgm_3_qgm_3 at 0x10f298f50&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title_song_cos_dlm_dc0_dlm_dc0</td>\n",
       "      <td>title</td>\n",
       "      <td>song</td>\n",
       "      <td>dlm_dc0</td>\n",
       "      <td>dlm_dc0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>&lt;function title_song_cos_dlm_dc0_dlm_dc0 at 0x10f298de8&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>title_song_jac_dlm_dc0_dlm_dc0</td>\n",
       "      <td>title</td>\n",
       "      <td>song</td>\n",
       "      <td>dlm_dc0</td>\n",
       "      <td>dlm_dc0</td>\n",
       "      <td>jaccard</td>\n",
       "      <td>&lt;function title_song_jac_dlm_dc0_dlm_dc0 at 0x121010c80&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>title_song_mel</td>\n",
       "      <td>title</td>\n",
       "      <td>song</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>monge_elkan</td>\n",
       "      <td>&lt;function title_song_mel at 0x121010c08&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>title_song_lev_dist</td>\n",
       "      <td>title</td>\n",
       "      <td>song</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>lev_dist</td>\n",
       "      <td>&lt;function title_song_lev_dist at 0x121010b90&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>title_song_lev_sim</td>\n",
       "      <td>title</td>\n",
       "      <td>song</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>lev_sim</td>\n",
       "      <td>&lt;function title_song_lev_sim at 0x121010d70&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>title_song_nmw</td>\n",
       "      <td>title</td>\n",
       "      <td>song</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>needleman_wunsch</td>\n",
       "      <td>&lt;function title_song_nmw at 0x120f20050&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>title_song_sw</td>\n",
       "      <td>title</td>\n",
       "      <td>song</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>smith_waterman</td>\n",
       "      <td>&lt;function title_song_sw at 0x120f200c8&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>artist_name_artists_jac_qgm_3_qgm_3</td>\n",
       "      <td>artist_name</td>\n",
       "      <td>artists</td>\n",
       "      <td>qgm_3</td>\n",
       "      <td>qgm_3</td>\n",
       "      <td>jaccard</td>\n",
       "      <td>&lt;function artist_name_artists_jac_qgm_3_qgm_3 at 0x120f20140&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>artist_name_artists_cos_dlm_dc0_dlm_dc0</td>\n",
       "      <td>artist_name</td>\n",
       "      <td>artists</td>\n",
       "      <td>dlm_dc0</td>\n",
       "      <td>dlm_dc0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>&lt;function artist_name_artists_cos_dlm_dc0_dlm_dc0 at 0x120f201b8&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>artist_name_artists_jac_dlm_dc0_dlm_dc0</td>\n",
       "      <td>artist_name</td>\n",
       "      <td>artists</td>\n",
       "      <td>dlm_dc0</td>\n",
       "      <td>dlm_dc0</td>\n",
       "      <td>jaccard</td>\n",
       "      <td>&lt;function artist_name_artists_jac_dlm_dc0_dlm_dc0 at 0x120f20230&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>artist_name_artists_mel</td>\n",
       "      <td>artist_name</td>\n",
       "      <td>artists</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>monge_elkan</td>\n",
       "      <td>&lt;function artist_name_artists_mel at 0x120f202a8&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>artist_name_artists_lev_dist</td>\n",
       "      <td>artist_name</td>\n",
       "      <td>artists</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>lev_dist</td>\n",
       "      <td>&lt;function artist_name_artists_lev_dist at 0x120f20320&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>artist_name_artists_lev_sim</td>\n",
       "      <td>artist_name</td>\n",
       "      <td>artists</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>lev_sim</td>\n",
       "      <td>&lt;function artist_name_artists_lev_sim at 0x120f20398&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>artist_name_artists_nmw</td>\n",
       "      <td>artist_name</td>\n",
       "      <td>artists</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>needleman_wunsch</td>\n",
       "      <td>&lt;function artist_name_artists_nmw at 0x120f20410&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>artist_name_artists_sw</td>\n",
       "      <td>artist_name</td>\n",
       "      <td>artists</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>smith_waterman</td>\n",
       "      <td>&lt;function artist_name_artists_sw at 0x120f20488&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>year_year_exm</td>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>exact_match</td>\n",
       "      <td>&lt;function year_year_exm at 0x120f20500&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>year_year_anm</td>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>abs_norm</td>\n",
       "      <td>&lt;function year_year_anm at 0x120f20578&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>year_year_lev_dist</td>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>lev_dist</td>\n",
       "      <td>&lt;function year_year_lev_dist at 0x120f205f0&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>year_year_lev_sim</td>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>lev_sim</td>\n",
       "      <td>&lt;function year_year_lev_sim at 0x120f20668&gt;</td>\n",
       "      <td>from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               feature_name left_attribute right_attribute  \\\n",
       "0                title_song_jac_qgm_3_qgm_3          title            song   \n",
       "1            title_song_cos_dlm_dc0_dlm_dc0          title            song   \n",
       "2            title_song_jac_dlm_dc0_dlm_dc0          title            song   \n",
       "3                            title_song_mel          title            song   \n",
       "4                       title_song_lev_dist          title            song   \n",
       "5                        title_song_lev_sim          title            song   \n",
       "6                            title_song_nmw          title            song   \n",
       "7                             title_song_sw          title            song   \n",
       "8       artist_name_artists_jac_qgm_3_qgm_3    artist_name         artists   \n",
       "9   artist_name_artists_cos_dlm_dc0_dlm_dc0    artist_name         artists   \n",
       "10  artist_name_artists_jac_dlm_dc0_dlm_dc0    artist_name         artists   \n",
       "11                  artist_name_artists_mel    artist_name         artists   \n",
       "12             artist_name_artists_lev_dist    artist_name         artists   \n",
       "13              artist_name_artists_lev_sim    artist_name         artists   \n",
       "14                  artist_name_artists_nmw    artist_name         artists   \n",
       "15                   artist_name_artists_sw    artist_name         artists   \n",
       "16                            year_year_exm           year            year   \n",
       "17                            year_year_anm           year            year   \n",
       "18                       year_year_lev_dist           year            year   \n",
       "19                        year_year_lev_sim           year            year   \n",
       "\n",
       "   left_attr_tokenizer right_attr_tokenizer       simfunction  \\\n",
       "0                qgm_3                qgm_3           jaccard   \n",
       "1              dlm_dc0              dlm_dc0            cosine   \n",
       "2              dlm_dc0              dlm_dc0           jaccard   \n",
       "3                 None                 None       monge_elkan   \n",
       "4                 None                 None          lev_dist   \n",
       "5                 None                 None           lev_sim   \n",
       "6                 None                 None  needleman_wunsch   \n",
       "7                 None                 None    smith_waterman   \n",
       "8                qgm_3                qgm_3           jaccard   \n",
       "9              dlm_dc0              dlm_dc0            cosine   \n",
       "10             dlm_dc0              dlm_dc0           jaccard   \n",
       "11                None                 None       monge_elkan   \n",
       "12                None                 None          lev_dist   \n",
       "13                None                 None           lev_sim   \n",
       "14                None                 None  needleman_wunsch   \n",
       "15                None                 None    smith_waterman   \n",
       "16                None                 None       exact_match   \n",
       "17                None                 None          abs_norm   \n",
       "18                None                 None          lev_dist   \n",
       "19                None                 None           lev_sim   \n",
       "\n",
       "                                                             function  \\\n",
       "0                <function title_song_jac_qgm_3_qgm_3 at 0x10f298f50>   \n",
       "1            <function title_song_cos_dlm_dc0_dlm_dc0 at 0x10f298de8>   \n",
       "2            <function title_song_jac_dlm_dc0_dlm_dc0 at 0x121010c80>   \n",
       "3                            <function title_song_mel at 0x121010c08>   \n",
       "4                       <function title_song_lev_dist at 0x121010b90>   \n",
       "5                        <function title_song_lev_sim at 0x121010d70>   \n",
       "6                            <function title_song_nmw at 0x120f20050>   \n",
       "7                             <function title_song_sw at 0x120f200c8>   \n",
       "8       <function artist_name_artists_jac_qgm_3_qgm_3 at 0x120f20140>   \n",
       "9   <function artist_name_artists_cos_dlm_dc0_dlm_dc0 at 0x120f201b8>   \n",
       "10  <function artist_name_artists_jac_dlm_dc0_dlm_dc0 at 0x120f20230>   \n",
       "11                  <function artist_name_artists_mel at 0x120f202a8>   \n",
       "12             <function artist_name_artists_lev_dist at 0x120f20320>   \n",
       "13              <function artist_name_artists_lev_sim at 0x120f20398>   \n",
       "14                  <function artist_name_artists_nmw at 0x120f20410>   \n",
       "15                   <function artist_name_artists_sw at 0x120f20488>   \n",
       "16                            <function year_year_exm at 0x120f20500>   \n",
       "17                            <function year_year_anm at 0x120f20578>   \n",
       "18                       <function year_year_lev_dist at 0x120f205f0>   \n",
       "19                        <function year_year_lev_sim at 0x120f20668>   \n",
       "\n",
       "                                                                                        function_source  \\\n",
       "0   from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "1   from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "2   from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "3   from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "4   from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "5   from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "6   from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "7   from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "8   from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "9   from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "10  from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "11  from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "12  from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "13  from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "14  from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "15  from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "16  from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "17  from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "18  from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "19  from py_entitymatching.feature.simfunctions import *\\nfrom py_entitymatching.feature.tokenizers ...   \n",
       "\n",
       "   is_auto_generated  \n",
       "0               True  \n",
       "1               True  \n",
       "2               True  \n",
       "3               True  \n",
       "4               True  \n",
       "5               True  \n",
       "6               True  \n",
       "7               True  \n",
       "8               True  \n",
       "9               True  \n",
       "10              True  \n",
       "11              True  \n",
       "12              True  \n",
       "13              True  \n",
       "14              True  \n",
       "15              True  \n",
       "16              True  \n",
       "17              True  \n",
       "18              True  \n",
       "19              True  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atypes1 = em.get_attr_types(sample_A)\n",
    "atypes2 = em.get_attr_types(sample_B)\n",
    "block_f = em.get_features(sample_A, sample_B, atypes1, atypes2, block_c, block_t, block_s)\n",
    "block_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[                              ]0%                          100%\n",
      "[##############################] | ETA: 00:06:09 | ETA: 00:05:54 | ETA: 00:05:37 | ETA: 00:05:17 | ETA: 00:05:01 | ETA: 00:04:46 | ETA: 00:04:32 | ETA: 00:04:19 | ETA: 00:04:10 | ETA: 00:03:59 | ETA: 00:03:47 | ETA: 00:03:34 | ETA: 00:03:22 | ETA: 00:03:09 | ETA: 00:02:57 | ETA: 00:02:45 | ETA: 00:02:32 | ETA: 00:02:20 | ETA: 00:02:08 | ETA: 00:01:57 | ETA: 00:01:45 | ETA: 00:01:33 | ETA: 00:01:21 | ETA: 00:01:09 | ETA: 00:00:58 | ETA: 00:00:49 | ETA: 00:00:36 | ETA: 00:00:24 | ETA: 00:00:12 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:06:11\n"
     ]
    }
   ],
   "source": [
    "# 3-gram Jaccard on song and title\n",
    "rule1 = ['title_song_jac_qgm_3_qgm_3(ltuple, rtuple) < 0.5']\n",
    "rb = em.RuleBasedBlocker()\n",
    "rb.add_rule(rule1, block_f, rule_name='rule_1')\n",
    "D = rb.block_candset(C)\n",
    "D.to_csv(datasets_dir + os.sep + \"D.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Debug rule-based blocking (3-gram Jaccard) on C\n",
    "dbg2 = em.debug_blocker(D, sA, sB, output_size=200, attr_corres=corres)\n",
    "dbg2.to_csv(datasets_dir + os.sep + \"dbg2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:06 | ETA: 00:00:06 | ETA: 00:00:06 | ETA: 00:00:06 | ETA: 00:00:06 | ETA: 00:00:06 | ETA: 00:00:05 | ETA: 00:00:05 | ETA: 00:00:05 | ETA: 00:00:04 | ETA: 00:00:04 | ETA: 00:00:04 | ETA: 00:00:04 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    }
   ],
   "source": [
    "# D has too many matches removed so try going back\n",
    "# and doing overlap blocking on songs in C1 with\n",
    "# more stop words\n",
    "stop = ['de', 'del', 'du', 'of', 'la', 'le']\n",
    "\n",
    "for word in stop:\n",
    "    ob.stop_words.append(word)\n",
    "    \n",
    "E = ob.block_candset(C1, 'title', 'song', rem_stop_words=True, verbose=True)\n",
    "\n",
    "E.to_csv(datasets_dir + os.sep + \"E.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'by',\n",
       " 'for',\n",
       " 'from',\n",
       " 'has',\n",
       " 'he',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'on',\n",
       " 'that',\n",
       " 'the',\n",
       " 'to',\n",
       " 'was',\n",
       " 'were',\n",
       " 'will',\n",
       " 'with',\n",
       " 'de',\n",
       " 'del',\n",
       " 'du',\n",
       " 'of',\n",
       " 'la',\n",
       " 'le',\n",
       " 'de',\n",
       " 'del',\n",
       " 'du',\n",
       " 'of',\n",
       " 'la',\n",
       " 'le',\n",
       " 'de',\n",
       " 'del',\n",
       " 'du',\n",
       " 'of',\n",
       " 'la',\n",
       " 'le']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:07 | ETA: 00:00:07 | ETA: 00:00:06 | ETA: 00:00:06 | ETA: 00:00:05 | ETA: 00:00:05 | ETA: 00:00:05 | ETA: 00:00:05 | ETA: 00:00:05 | ETA: 00:00:04 | ETA: 00:00:04 | ETA: 00:00:04 | ETA: 00:00:04 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    }
   ],
   "source": [
    "# Title-Song 1 word overlap blocking on C1 with more stop words added\n",
    "stop2 = ['i', 'im', 'my', 'me', 'you', 'your', 'we', 'our', 'el', 'after', 'theme', 'y', 'that', 'like', 'little', 'all', 'love']\n",
    "\n",
    "for word in stop2:\n",
    "    ob.stop_words.append(word)\n",
    "    \n",
    "ob.stop_words = list(set(ob.stop_words))\n",
    "\n",
    "E1 = ob.block_candset(C1, 'title', 'song', rem_stop_words=True, verbose=True)\n",
    "\n",
    "E1.to_csv(datasets_dir + os.sep + \"E1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Debug title-song overlap blocker on C1\n",
    "dbg3 = em.debug_blocker(E1, sA, sB, output_size=200, attr_corres=corres)\n",
    "dbg3.to_csv(datasets_dir + os.sep + \"dbg3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "# Modify sample_B2 to remove \"+\" between artist names and replace with a single whitespace\n",
    "# to improve recall\n",
    "# Rerun 1-word overlap blocking on artist_name and artists\n",
    "\n",
    "# Read and set metadata for downsampled B1 (B with '+' removed from artists)\n",
    "sB1 = em.read_csv_metadata(datasets_dir + os.sep + \"sampleB1.csv\", key='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "C1a = ob.block_tables(sA, sB1, 'artist_name', 'artists', \n",
    "                     rem_stop_words=True, word_level=True, overlap_size=1, \n",
    "                     l_output_attrs=['id', 'title', 'artist_name', 'year'], \n",
    "                     r_output_attrs=['id', 'title', 'year', 'episode', 'song', 'artists'],\n",
    "                     show_progress=True)\n",
    "\n",
    "C1a.to_csv(datasets_dir + os.sep + \"C1a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:09 | ETA: 00:00:08 | ETA: 00:00:08 | ETA: 00:00:08 | ETA: 00:00:08 | ETA: 00:00:08 | ETA: 00:00:08 | ETA: 00:00:07 | ETA: 00:00:07 | ETA: 00:00:06 | ETA: 00:00:06 | ETA: 00:00:06 | ETA: 00:00:05 | ETA: 00:00:05 | ETA: 00:00:04 | ETA: 00:00:04 | ETA: 00:00:04 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:03 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:09\n"
     ]
    }
   ],
   "source": [
    "# Title-Song 1 word overlap blocking on C1a\n",
    "C1b = ob.block_candset(C1a, 'title', 'song', rem_stop_words=True, verbose=True)\n",
    "\n",
    "C1b.to_csv(datasets_dir + os.sep + \"C1b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Debug title-song overlap blocker on C1\n",
    "dbg4 = em.debug_blocker(C1b, sA, sB1, output_size=200, attr_corres=corres)\n",
    "dbg4.to_csv(datasets_dir + os.sep + \"dbg4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample candidate set\n",
    "S1 = em.sample_table(C1b, 450)\n",
    "S1.to_csv(datasets_dir + os.sep + 'S1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Too many non-matches in C1b\n",
    "# Add rule to check if title begin with the same word\n",
    "# Check against the first two words in case one title starts with\n",
    "# an article and the other omits it (ex. \"The New Machine\" and \"New Machine\")\n",
    "def title_song_function(x, y):\n",
    "    if len(x)==0 or len(y)==(0):\n",
    "        return True\n",
    "    \n",
    "    x_title = x['title'].lower().split()\n",
    "    y_title = y['song'].lower().split()\n",
    "    \n",
    "    x1 = x_title[0]\n",
    "    \n",
    "    x2 = ''\n",
    "    if len(x_title) > 1:\n",
    "        x2 = x_title[1]\n",
    "        \n",
    "    y1 = y_title[0]\n",
    "    \n",
    "    y2 = ''\n",
    "    if len(y_title) > 1:\n",
    "        y2 = y_title[1]\n",
    "    \n",
    "    if (x1 == y1 or x1 == y2 or x2 == y1 or x2 == y2):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb = em.BlackBoxBlocker()\n",
    "bb.set_black_box_function(title_song_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:02 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    }
   ],
   "source": [
    "C3 = bb.block_candset(C1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Debug title-song overlap blocker on C3\n",
    "dbg5 = em.debug_blocker(C3, sA, sB1, output_size=200, attr_corres=corres)\n",
    "dbg5.to_csv(datasets_dir + os.sep + \"dbg5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample candidate set\n",
    "S2 = em.sample_table(C3, 450)\n",
    "S2.to_csv(datasets_dir + os.sep + 'S2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n",
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Missing some matches in C3\n",
    "# Create another overlap block on 2-word overlap on song\n",
    "# and 1 word overlap on artist\n",
    "C4 = ob.block_tables(sA, sB1, 'artist_name', 'artists', \n",
    "                     rem_stop_words=True, word_level=True, overlap_size=2, \n",
    "                     l_output_attrs=['id', 'title', 'artist_name', 'year'], \n",
    "                     r_output_attrs=['id', 'title', 'year', 'episode', 'song', 'artists'],\n",
    "                     show_progress=True)\n",
    "\n",
    "C4 = ob.block_candset(C4, 'title', 'song', \n",
    "                     rem_stop_words=True, word_level=True, overlap_size=2, \n",
    "                     show_progress=True)\n",
    "\n",
    "C4.to_csv(datasets_dir + os.sep + \"C4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine C3 and C4: 1-word overlap on artist and title and \n",
    "# either the one of the few words of the title match or \n",
    "# there is a 2-word overlap for the title\n",
    "C5 = em.combine_blocker_outputs_via_union([C3, C4])\n",
    "C5.to_csv(datasets_dir + os.sep + \"C5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Debug title-song overlap blocker on C5\n",
    "dbg6 = em.debug_blocker(C5, sA, sB1, output_size=200, attr_corres=corres)\n",
    "dbg6.to_csv(datasets_dir + os.sep + \"dbg6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample candidate set\n",
    "S3 = em.sample_table(C5, 400)\n",
    "S3.to_csv(datasets_dir + os.sep + 'S3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "# Read in labelled data\n",
    "path_G = datasets_dir + os.sep + \"G.csv\"\n",
    "G = em.read_csv_metadata(path_G, encoding='utf-8',\n",
    "                         key='_id',\n",
    "                         ltable=A, rtable=B, \n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split D into development set (I) and evaluation set (J)\n",
    "IJ = em.split_train_test(G, train_proportion=0.7, random_state=0)\n",
    "I = IJ['train']\n",
    "J = IJ['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a set of ML-matchers\n",
    "dt = em.DTMatcher(name='DecisionTree', random_state=0)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher(name='NaiveBayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def Unnamed:_0_Unnamed:_0_exm(ltuple, rtuple):\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Generate features\n",
    "# sample_A, sample_B, atypes1, atypes2, block_c, block_t, block_s\n",
    "atypes1 = em.get_attr_types(sA)\n",
    "atypes2 = em.get_attr_types(sB1)\n",
    "block_c = em.get_attr_corres(sA, sB1)\n",
    "F = em.get_features(sA, sB1, atypes1, atypes2, block_c, block_t, block_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete first column from each table to resolve error\n",
    "del sA['Unnamed: 0']\n",
    "del sB1['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate features\n",
    "# sample_A, sample_B, atypes1, atypes2, block_c, block_t, block_s\n",
    "atypes1 = em.get_attr_types(sA)\n",
    "atypes2 = em.get_attr_types(sB1)\n",
    "block_c = em.get_attr_corres(sA, sB1)\n",
    "block_c['corres'] = [('id', 'id'), ('title','song'), ('artist_name','artists'), ('year', 'year')]\n",
    "F = em.get_features(sA, sB1, atypes1, atypes2, block_c, block_t, block_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:04 | ETA: 00:00:03 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:02 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "# Convert the I into a set of feature vectors using F\n",
    "H = em.extract_feature_vecs(I, \n",
    "                            feature_table=F, \n",
    "                            attrs_after='gold',\n",
    "                            show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Matcher</th>\n",
       "      <th>Num folds</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Fold 4</th>\n",
       "      <th>Fold 5</th>\n",
       "      <th>Mean score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>&lt;py_entitymatching.matcher.dtmatcher.DTMatcher object at 0x121f2d690&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.873993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>&lt;py_entitymatching.matcher.rfmatcher.RFMatcher object at 0x121f2d490&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.939561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>&lt;py_entitymatching.matcher.svmmatcher.SVMMatcher object at 0x121f2d2d0&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>&lt;py_entitymatching.matcher.linregmatcher.LinRegMatcher object at 0x121f2d510&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.927188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>&lt;py_entitymatching.matcher.logregmatcher.LogRegMatcher object at 0x121f2de10&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.926200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>&lt;py_entitymatching.matcher.nbmatcher.NBMatcher object at 0x121f2d710&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.924872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name  \\\n",
       "0  DecisionTree   \n",
       "1            RF   \n",
       "2           SVM   \n",
       "3        LinReg   \n",
       "4        LogReg   \n",
       "5    NaiveBayes   \n",
       "\n",
       "                                                                         Matcher  \\\n",
       "0          <py_entitymatching.matcher.dtmatcher.DTMatcher object at 0x121f2d690>   \n",
       "1          <py_entitymatching.matcher.rfmatcher.RFMatcher object at 0x121f2d490>   \n",
       "2        <py_entitymatching.matcher.svmmatcher.SVMMatcher object at 0x121f2d2d0>   \n",
       "3  <py_entitymatching.matcher.linregmatcher.LinRegMatcher object at 0x121f2d510>   \n",
       "4  <py_entitymatching.matcher.logregmatcher.LogRegMatcher object at 0x121f2de10>   \n",
       "5          <py_entitymatching.matcher.nbmatcher.NBMatcher object at 0x121f2d710>   \n",
       "\n",
       "   Num folds    Fold 1    Fold 2    Fold 3  Fold 4    Fold 5  Mean score  \n",
       "0          5  0.692308  0.923077  0.913043    0.88  0.961538    0.873993  \n",
       "1          5  0.904762  0.913043  0.880000    1.00  1.000000    0.939561  \n",
       "2          5  1.000000  1.000000  0.928571    1.00  1.000000    0.985714  \n",
       "3          5  0.863636  0.925926  0.913043    1.00  0.933333    0.927188  \n",
       "4          5  0.863636  0.923077  0.880000    1.00  0.964286    0.926200  \n",
       "5          5  0.900000  0.916667  0.846154    1.00  0.961538    0.924872  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the best ML matcher using CV\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg, nb], table=H, \n",
    "        exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'],\n",
    "        k=5,\n",
    "        target_attr='gold', metric='precision', random_state=0)\n",
    "\n",
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Matcher</th>\n",
       "      <th>Num folds</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Fold 4</th>\n",
       "      <th>Fold 5</th>\n",
       "      <th>Mean score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>&lt;py_entitymatching.matcher.dtmatcher.DTMatcher object at 0x121f2d690&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.890479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>&lt;py_entitymatching.matcher.rfmatcher.RFMatcher object at 0x121f2d490&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.907212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>&lt;py_entitymatching.matcher.svmmatcher.SVMMatcher object at 0x121f2d2d0&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.532327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>&lt;py_entitymatching.matcher.linregmatcher.LinRegMatcher object at 0x121f2d510&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.935140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>&lt;py_entitymatching.matcher.logregmatcher.LogRegMatcher object at 0x121f2de10&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.937981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>&lt;py_entitymatching.matcher.nbmatcher.NBMatcher object at 0x121f2d710&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.884185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name  \\\n",
       "0  DecisionTree   \n",
       "1            RF   \n",
       "2           SVM   \n",
       "3        LinReg   \n",
       "4        LogReg   \n",
       "5    NaiveBayes   \n",
       "\n",
       "                                                                         Matcher  \\\n",
       "0          <py_entitymatching.matcher.dtmatcher.DTMatcher object at 0x121f2d690>   \n",
       "1          <py_entitymatching.matcher.rfmatcher.RFMatcher object at 0x121f2d490>   \n",
       "2        <py_entitymatching.matcher.svmmatcher.SVMMatcher object at 0x121f2d2d0>   \n",
       "3  <py_entitymatching.matcher.linregmatcher.LinRegMatcher object at 0x121f2d510>   \n",
       "4  <py_entitymatching.matcher.logregmatcher.LogRegMatcher object at 0x121f2de10>   \n",
       "5          <py_entitymatching.matcher.nbmatcher.NBMatcher object at 0x121f2d710>   \n",
       "\n",
       "   Num folds    Fold 1    Fold 2    Fold 3    Fold 4   Fold 5  Mean score  \n",
       "0          5  0.947368  0.923077  0.954545  0.846154  0.78125    0.890479  \n",
       "1          5  1.000000  0.807692  1.000000  0.884615  0.84375    0.907212  \n",
       "2          5  0.789474  0.500000  0.590909  0.500000  0.28125    0.532327  \n",
       "3          5  1.000000  0.961538  0.954545  0.884615  0.87500    0.935140  \n",
       "4          5  1.000000  0.923077  1.000000  0.923077  0.84375    0.937981  \n",
       "5          5  0.947368  0.846154  1.000000  0.846154  0.78125    0.884185  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = em.select_matcher([dt, rf, svm, ln, lg, nb], table=H, \n",
    "        exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'],\n",
    "        k=5,\n",
    "        target_attr='gold', metric='recall', random_state=0)\n",
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert J into a set of feature vectors using F\n",
    "L = em.extract_feature_vecs(J, feature_table=F, attrs_after='gold', show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test matchers\n",
    "models = [dt, svm, rf, lg, ln, nb]\n",
    "\n",
    "def train(models):\n",
    "    for model in models:\n",
    "        # Train using feature vectors from I \n",
    "        model.fit(table=H, \n",
    "        exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "        target_attr='gold')\n",
    "\n",
    "        # Predict on L \n",
    "        predictions = model.predict(table=L, exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "              append=True, target_attr='predicted', inplace=False)\n",
    "        \n",
    "        # Evaluate the predictions\n",
    "        print ('Predictions of ' + model.name + ' on J')\n",
    "        eval_result = em.eval_matches(predictions, 'gold', 'predicted')\n",
    "        em.print_eval_summary(eval_result)\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions of DecisionTree on J\n",
      "Precision : 92.86% (52/56)\n",
      "Recall : 88.14% (52/59)\n",
      "F1 : 90.43%\n",
      "False positives : 4 (out of 56 positive predictions)\n",
      "False negatives : 7 (out of 61 negative predictions)\n",
      "\n",
      "\n",
      "Predictions of SVM on J\n",
      "Precision : 100.0% (25/25)\n",
      "Recall : 42.37% (25/59)\n",
      "F1 : 59.52%\n",
      "False positives : 0 (out of 25 positive predictions)\n",
      "False negatives : 34 (out of 92 negative predictions)\n",
      "\n",
      "\n",
      "Predictions of RF on J\n",
      "Precision : 88.24% (45/51)\n",
      "Recall : 76.27% (45/59)\n",
      "F1 : 81.82%\n",
      "False positives : 6 (out of 51 positive predictions)\n",
      "False negatives : 14 (out of 66 negative predictions)\n",
      "\n",
      "\n",
      "Predictions of LogReg on J\n",
      "Precision : 92.73% (51/55)\n",
      "Recall : 86.44% (51/59)\n",
      "F1 : 89.47%\n",
      "False positives : 4 (out of 55 positive predictions)\n",
      "False negatives : 8 (out of 62 negative predictions)\n",
      "\n",
      "\n",
      "Predictions of LinReg on J\n",
      "Precision : 94.44% (51/54)\n",
      "Recall : 86.44% (51/59)\n",
      "F1 : 90.27%\n",
      "False positives : 3 (out of 54 positive predictions)\n",
      "False negatives : 8 (out of 63 negative predictions)\n",
      "\n",
      "\n",
      "Predictions of NaiveBayes on J\n",
      "Precision : 89.09% (49/55)\n",
      "Recall : 83.05% (49/59)\n",
      "F1 : 85.96%\n",
      "False positives : 6 (out of 55 positive predictions)\n",
      "False negatives : 10 (out of 62 negative predictions)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
